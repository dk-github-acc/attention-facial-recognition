{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Map Examples for Image Classification\n",
    "\n",
    "Much of the code in this blog was adapted from [Jacob Gil's Github repository for class activation mapping](https://github.com/jacobgil/keras-cam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist # load in our MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help with typing, we'll define some common shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "ImageShape = Tuple[int, int]\n",
    "GrayScaleImageShape = Tuple[int, int, int] # a grayscale image should have a H x W x 1 dimensionality\n",
    "Dataset = Tuple[np.ndarray, np.ndarray]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data and Train/Test Splits\n",
    "We first split the MNIST dataset into train and test splits, and print out their shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is (60000, 28, 28)\n",
      "The shape of y_train is (60000,)\n",
      "The shape of X_test is (10000, 28, 28)\n",
      "The shape of y_test is (10000,) - some example targets: [7 2 1 0 4]\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(f\"The shape of X_train is {X_train.shape}\")\n",
    "print(f\"The shape of y_train is {y_train.shape}\")\n",
    "print(f\"The shape of X_test is {X_test.shape}\")\n",
    "print(f\"The shape of y_test is {y_test.shape} - some example targets: {y_test[:5]}\")\n",
    "mnist_image_shape: ImageShape = X_train.shape[1:]\n",
    "print(mnist_image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encode Categorical Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding y_train (60000,) -> (60000, 10)\n",
      "One-hot encoding y_test (10000,) -> (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "OneHotEncodedTarget = np.ndarray\n",
    "Categories = int\n",
    "encoded_y_train: OneHotEncodedTarget = to_categorical(y_train)\n",
    "encoded_y_test: OneHotEncodedTarget = to_categorical(y_test)\n",
    "print(f\"One-hot encoding y_train {y_train.shape} -> {encoded_y_train.shape}\")\n",
    "print(f\"One-hot encoding y_test {y_test.shape} -> {encoded_y_test.shape}\")\n",
    "\n",
    "CLASSES: Categories = encoded_y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to reshape the tensors and expand their dimensionality in order for Keras to accept their tensor shape as input. Essentially, this involves adding an extra dimension to the current tensor shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding shape from (60000, 28, 28) to (60000, 28, 28, 1)\n",
      "Expanding shape from (10000, 28, 28) to (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def expand_tensor_shape(X_train: np.ndarray)-> np.ndarray:\n",
    "    new_shape: Tuple = X_train.shape + (1,)\n",
    "    new_tensor = X_train.reshape(new_shape)\n",
    "    print(f\"Expanding shape from {X_train.shape} to {new_tensor.shape}\")\n",
    "    return new_tensor\n",
    "\n",
    "X_train_expanded: np.ndarray = expand_tensor_shape(X_train)\n",
    "X_test_expanded: np.ndarray = expand_tensor_shape(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Vanilla CNN Implementation (With No Attention / Class-Activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanilla architecture we will build as a baseline \"Hello World\" contains two 2D Convolutional layers, followed by a `Flatten` aggregation layer:\n",
    "<img src=\"public/vanilla_architecture.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_5/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 24, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                184330    \n",
      "=================================================================\n",
      "Total params: 203,434.0\n",
      "Trainable params: 203,434.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# define model architecture and hyperparameters\n",
    "NUM_FILTERS_L1 = 64\n",
    "NUM_FILTERS_L2 = 32\n",
    "KERNEL_SIZE = 3\n",
    "\n",
    "# the images are 28 x 28 (pixel size) x 1 (grayscale - if RGB, then 3)\n",
    "input_dims: GrayScaleImageShape = (28,28,1)\n",
    "\n",
    "def build_vanilla_cnn(filters_layer1:int, filters_layer2:int, kernel_size:int, input_dims: GrayScaleImageShape)-> Model:\n",
    "    inputs: Tensor = Input(shape=input_dims)\n",
    "    x: Tensor = Conv2D(filters=filters_layer1, kernel_size=kernel_size, activation='relu')(inputs)\n",
    "    x: Tensor = Conv2D(filters=filters_layer2, kernel_size=kernel_size, activation='relu')(x)\n",
    "    x: Tensor = Flatten()(x)\n",
    "    predictions = Dense(CLASSES, activation=\"softmax\")(x)\n",
    "    print(predictions)\n",
    "\n",
    "    #compile model using accuracy to measure model performance\n",
    "    model: Model = Model(inputs=inputs, outputs=predictions)\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model: Model = build_vanilla_cnn(NUM_FILTERS_L1, NUM_FILTERS_L2, KERNEL_SIZE, input_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Run the Model\n",
    "Keras will return a history object from the `model.fit()` call. This object contains training loss, validation loss, and the performance metrics you've selected to evaluate (in this case accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History\n",
    "\n",
    "RUN_VANILLA = False # set this to True to actually run the model\n",
    "\n",
    "if RUN_VANILLA:\n",
    "    history: History = model.fit(X_train_expanded, encoded_y_train, \n",
    "                                 validation_data=(X_test_expanded, encoded_y_test), epochs=2, batch_size=2058)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Class Activation Map for the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, MaxPool2D, Layer, Lambda\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "import cv2\n",
    "\n",
    "def global_average_pooling(x: Layer):\n",
    "    return K.mean(x, axis = (2,3))\n",
    "\n",
    "def global_average_pooling_shape(input_shape):\n",
    "    # return the dimensions corresponding with batch size and number of filters\n",
    "    return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def build_global_average_pooling_layer(function, output_shape):\n",
    "    return Lambda(pooling_function, output_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture for Basic Class Activation Mapped CNN\n",
    "<img src=\"public/cam_architecture.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "The original `28x28` image has a depth of 1 (grayscale) and is mapped to 32 filter feature maps of size `24x24`. These are then aggregated in the global average pooling layer to 1 scalar value per feature map (32 feature maps in total $\\rightarrow$ 32 scalar averages. These are then run through a dense softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_class_activation_map_cnn()-> Model:\n",
    "    '''\n",
    "    I've commented out different internal layers of the CNN architecture. However, feel free to comment them back in to\n",
    "    add more complexity to the model. Note that doing so may dramatically increase training time.\n",
    "    '''\n",
    "\n",
    "    inputs: Tensor = Input(shape=(28,28,1))\n",
    "    x: Tensor = Conv2D(filters=32, \n",
    "                       kernel_size=5, \n",
    "                       activation='relu',\n",
    "                       name='final_convolution_layer')(inputs)\n",
    "    # x: Tensor = MaxPool2D()(x)\n",
    "    # x: Tensor = Conv2D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "    x: Tensor = Lambda(lambda x: K.mean(x, axis=(1,2)), output_shape=global_average_pooling_shape)(x)\n",
    "    # x: Tensor = Dense(128, activation=\"relu\")(x)\n",
    "    predictions: Tensor = Dense(10, activation=\"softmax\")(x)\n",
    "    model: Model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def run_cnn_cam(model, save_filepath):\n",
    "    history: History = model.fit(X_train_expanded, encoded_y_train, \n",
    "                                 validation_data=(X_test_expanded, encoded_y_test), epochs=100, batch_size=5126)\n",
    "\n",
    "    model.save(save_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "final_convolution_layer (Con (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,162.0\n",
      "Trainable params: 1,162.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x18b304006a0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a dry run of the model construction to see its architecture and parameter distributions\n",
    "build_class_activation_map_cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we purposely gave the final `Conv2D` layer an explicit name: `final_convolution_layer` - we do this so that after our model has finished training, we can explicitly call `model.get_layer(final_convolution_layer)` to grab that layer's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_models\\basic_cam.h5\n"
     ]
    }
   ],
   "source": [
    "RUN_CAM = False\n",
    "MODEL_FOLDER = \"saved_models\"\n",
    "save_filepath: str = \"basic_cam.h5\"\n",
    "    \n",
    "import os\n",
    "saved_model_relative_path: str = os.path.join(MODEL_FOLDER, save_filepath)\n",
    "print(saved_model_relative_path)\n",
    "if RUN_CAM:\n",
    "    cam_model = build_class_activation_map_cnn()\n",
    "    run_cnn_cam(cam_model, saved_model_relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Persisted Model and Visualize Class Activation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "warnings.filterwarnings('ignore')\n",
    "model = load_model(saved_model_relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function to Grab Outputs of a Particular Keras Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two main inputs:\n",
    "\n",
    "1. The `class_weights`, which should be of dimension size `number of nodes in final feedforward layer x number of classes`. This is easy to get because we can simply call `model.layers[-1]` to retrieve the last layer in the model architecture, and then call `get_weights` to fetch the Numpy array of weights. Note that `get_weights` actually returns a list of two weight matrices - one for the incoming weights (which is what we care about) and the other the outbound weights (which aren't as useful for us in this case).\n",
    "\n",
    "\n",
    "2. The **output of the final convolutional layer**. Since our model could be of any arbitrary depth, we need to get this particular layer by its name, which - if you remember - we conveniently labelled `final_convolutional_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layer(model: Model, layer_name: str)-> Layer:\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "    layer = layer_dict[layer_name]\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Class Activation Maps for One Sample Image\n",
    "We'll randomly pick the 6th image of the dataset and visualize it. It happens to be a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18b35069ba8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_image = X_train[5]\n",
    "first_image = first_image.reshape(28,28,1)\n",
    "img = np.array(first_image).reshape(1, 28, 28, 1)\n",
    "img.shape\n",
    "plt.imshow(img.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final convolution layer has outputs of (24, 24, 32)\n",
      "Final dense layer class weights have output of (32, 10)\n"
     ]
    }
   ],
   "source": [
    "class_weights = model.layers[-1].get_weights()[0]\n",
    "\n",
    "conv_layer_name = \"conv2d_1\" # I accidentally saved the model with this as the conv2d layer name - oops!\n",
    "final_conv_layer = get_output_layer(model, \"conv2d_1\")\n",
    "\n",
    "# define a Keras function to accept as input the model image input and return the final dense layer weights and\n",
    "# convolution layer weights\n",
    "get_output = K.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n",
    "\n",
    "[conv_outputs, predictions] = get_output([img])\n",
    "conv_outputs = conv_outputs[0,:,:,:]\n",
    "print(f\"Final convolution layer has outputs of {conv_outputs.shape}\")\n",
    "print(f\"Final dense layer class weights have output of {class_weights.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function to Generate Class Activation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cam(conv_outputs: np.ndarray, class_weights: np.ndarray, original_shape: Tuple[int, int], target_class: int):\n",
    "    \n",
    "    '''\n",
    "    Our class activation map should be the same shape as the first two dimensions of our Conv2D output (24 x 24).\n",
    "    We finally also need to interpolate our CAM back to the original image (which we do with cv2.resize)\n",
    "    '''\n",
    "    \n",
    "    cam = np.zeros(dtype=np.float32, shape = conv_outputs.shape[0:2])\n",
    "\n",
    "    # we select ONLY the weight vectors from our class weight matrix (of size 32 x 10) that apply to the target class\n",
    "    # that we care about (in this case, 2)\n",
    "    for i, w in enumerate(class_weights[:, target_class]):\n",
    "        cam += w * conv_outputs[:,:,i]\n",
    "    cam /= np.max(cam)\n",
    "    return cv2.resize(cam, original_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Generate a Heatmap from Class Activation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmap(cam: np.ndarray)-> np.ndarray:\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "    heatmap[np.where(cam < 0.1)] = 0\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Heatmaps for True Class and a Few False Classes to See Where the CNN Model Is \"Focusing\" On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = make_cam(conv_outputs, class_weights, original_shape=(28,28), target_class=2)\n",
    "false_cam = make_cam(conv_outputs, class_weights, original_shape=(28,28), target_class=4)\n",
    "false2_cam = make_cam(conv_outputs, class_weights, original_shape=(28,28), target_class=5)\n",
    "\n",
    "heatmap = make_heatmap(cam)\n",
    "false_heatmap = make_heatmap(false_cam)\n",
    "false2_heatmap = make_heatmap(false2_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'True Image')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_img = heatmap*0.5 + img\n",
    "final_img = new_img.reshape((28,28,3))\n",
    "\n",
    "imgs = [heatmap, img.reshape(28,28)]\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 15))\n",
    "axes[0].imshow(heatmap)\n",
    "axes[0].set_title(\"Activation Map for 2\")\n",
    "axes[1].imshow(false_heatmap)\n",
    "axes[1].set_title(\"Activation Map for 4\")\n",
    "axes[2].imshow(false2_heatmap)\n",
    "axes[2].set_title(\"Activation Map for 5\")\n",
    "axes[3].imshow(img.reshape((28,28)))\n",
    "axes[3].set_title(\"True Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
