{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History\n",
    "from keras import models, layers\n",
    "from keras import backend as K\n",
    "from skimage.transform import resize\n",
    "from keras.optimizers import Adam\n",
    "import json\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, MaxPooling2D, Layer, Lambda\n",
    "\n",
    "import warnings\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Tuple\n",
    "from keras.preprocessing.image import load_img\n",
    "import boto3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = int(480/12)\n",
    "HEIGHT = int(640/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Get Mappings Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mappings(bucket_name: str, mapping_path: str) -> Dict[str, str]:\n",
    "    s3 = boto3.resource('s3')\n",
    "    faces_bucket = s3.Bucket(bucket_name)  # instantiate the bucket object\n",
    "\n",
    "    obj = s3.Object(bucket_name, mapping_path)  # fetch the mapping dictionary\n",
    "\n",
    "    json_string: str = obj.get()['Body'].read().decode('utf-8')\n",
    "    mappings_dict: Dict[str, str] = json.loads(\n",
    "        json_string)  # this mappings_dict contains filename -> gender class mapping\n",
    "    print(list(mappings_dict.items())[:3])  # print the first three entries of the mappings dictionary\n",
    "    return mappings_dict\n",
    "\n",
    "\n",
    "def expand_tensor_shape(X_train: np.ndarray) -> np.ndarray:\n",
    "    new_shape: Tuple = X_train.shape + (1,)\n",
    "    new_tensor = X_train.reshape(new_shape)\n",
    "    print(f\"Expanding shape from {X_train.shape} to {new_tensor.shape}\")\n",
    "    return new_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is /home/ubuntu/attention-facial-recognition\n",
      "[('1-01.jpg', 'male'), ('1-02.jpg', 'male'), ('1-03.jpg', 'male')]\n",
      "Downloading 1-11.jpg, saving as /home/ubuntu/attention-facial-recognition/faces/1-11.jpg\n",
      "An error occurred (404) when calling the HeadObject operation: Not Found:Error downloading 1-11.jpg\n",
      "Downloading 1-12.jpg, saving as /home/ubuntu/attention-facial-recognition/faces/1-12.jpg\n",
      "An error occurred (404) when calling the HeadObject operation: Not Found:Error downloading 1-12.jpg\n",
      "Downloading 1-13.jpg, saving as /home/ubuntu/attention-facial-recognition/faces/1-13.jpg\n",
      "An error occurred (404) when calling the HeadObject operation: Not Found:Error downloading 1-13.jpg\n",
      "Downloading 10-02.jpg, saving as /home/ubuntu/attention-facial-recognition/faces/10-02.jpg\n",
      "An error occurred (404) when calling the HeadObject operation: Not Found:Error downloading 10-02.jpg\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "warnings.filterwarnings('ignore')\n",
    "S3_BUCKET_NAME = \"fei-faces-sao-paulo\"\n",
    "mapping = 'classification/gender.json'\n",
    "\n",
    "print(f\"Current working directory is {cwd}\")\n",
    "s3 = boto3.client('s3')\n",
    "warnings.filterwarnings('ignore')\n",
    "IMAGE_LIMIT = 3000\n",
    "LOCAL_IMAGES_FOLDER = \"faces\"\n",
    "\n",
    "mappings_dict: Dict[str, str] = get_mappings(S3_BUCKET_NAME, mapping)\n",
    "\n",
    "target: List[str] = []  # this list will contain our actual tensors (as N-dimensional numpy arrays)\n",
    "images: List[np.ndarray] = []  # this list will contain our classes (male or female)\n",
    "\n",
    "for filename, gender in mappings_dict.items():\n",
    "\n",
    "    if \"-14\" in filename or \"-10\" in filename:  # these images are blurry or obscured\n",
    "        continue\n",
    "\n",
    "    local_filename: str = os.path.join(cwd, LOCAL_IMAGES_FOLDER, filename)\n",
    "    try:\n",
    "        if not os.path.isfile(local_filename):  # if file does not exist locally\n",
    "            print(f\"Downloading {filename}, saving as {local_filename}\")\n",
    "            s3.download_file(S3_BUCKET_NAME, filename, local_filename)\n",
    "        else:\n",
    "            logging.debug(f\"Found a local copy of {local_filename}\")\n",
    "\n",
    "        # use the Keras image API to load in an image\n",
    "        img = load_img(local_filename)\n",
    "        #print(\"Resizing\")\n",
    "        img = img.resize((HEIGHT, WIDTH))\n",
    "        #img = img.convert('L')  # convert o gray scale\n",
    "        # report details about the image\n",
    "        images.append(np.array(img))\n",
    "        target.append(gender)\n",
    "        if len(images) == IMAGE_LIMIT:\n",
    "            print(\"Breaking after reaching image limit.\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"{e}:Error downloading {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Target to Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding target vector (2396,) -> (2396, 2)\n",
      "There are 2 classes to predict.\n"
     ]
    }
   ],
   "source": [
    "binary_target = np.array(list(map(lambda gender: 0 if gender == 'male' else 1, target)))\n",
    "encoded_target = to_categorical(binary_target)\n",
    "\n",
    "print(f\"One-hot encoding target vector {binary_target.shape} -> {encoded_target.shape}\")\n",
    "NUM_CLASSSES = encoded_target.shape[1]\n",
    "print(f\"There are {NUM_CLASSSES} classes to predict.\")\n",
    "\n",
    "indices = np.linspace(0, len(binary_target) - 1, len(binary_target))\n",
    "validation_indices = np.random.choice(indices, size=int(len(binary_target) * 0.3), replace=False).astype(int)\n",
    "training_indices = set(indices).difference(set(validation_indices))\n",
    "training_indices = np.array(list(training_indices)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Neural Network VGG Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "_KERAS_BACKEND = keras.backend\n",
    "_KERAS_LAYERS = keras.layers\n",
    "_KERAS_MODELS = keras.models\n",
    "_KERAS_UTILS = keras.utils\n",
    "\n",
    "\n",
    "WEIGHTS_PATH = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                'releases/download/v0.1/'\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "WEIGHTS_PATH_NO_TOP = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                       'releases/download/v0.1/'\n",
    "                       'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "def get_submodules_from_kwargs(kwargs):\n",
    "    backend = kwargs.get('backend', _KERAS_BACKEND)\n",
    "    layers = kwargs.get('layers', _KERAS_LAYERS)\n",
    "    models = kwargs.get('models', _KERAS_MODELS)\n",
    "    utils = kwargs.get('utils', _KERAS_UTILS)\n",
    "    for key in kwargs.keys():\n",
    "        if key not in ['backend', 'layers', 'models', 'utils']:\n",
    "            raise TypeError('Invalid keyword argument: %s', key)\n",
    "    return backend, layers, models, utils\n",
    "\n",
    "def _obtain_input_shape(input_shape,\n",
    "                        default_size,\n",
    "                        min_size,\n",
    "                        data_format,\n",
    "                        require_flatten,\n",
    "                        weights=None):\n",
    "    \"\"\"Internal utility to compute/validate a model's input shape.\n",
    "    # Arguments\n",
    "        input_shape: Either None (will return the default network input shape),\n",
    "            or a user-provided shape to be validated.\n",
    "        default_size: Default input width/height for the model.\n",
    "        min_size: Minimum input width/height accepted by the model.\n",
    "        data_format: Image data format to use.\n",
    "        require_flatten: Whether the model is expected to\n",
    "            be linked to a classifier via a Flatten layer.\n",
    "        weights: One of `None` (random initialization)\n",
    "            or 'imagenet' (pre-training on ImageNet).\n",
    "            If weights='imagenet' input channels must be equal to 3.\n",
    "    # Returns\n",
    "        An integer shape tuple (may include None entries).\n",
    "    # Raises\n",
    "        ValueError: In case of invalid argument values.\n",
    "    \"\"\"\n",
    "    if weights != 'imagenet' and input_shape and len(input_shape) == 3:\n",
    "        if data_format == 'channels_first':\n",
    "            if input_shape[0] not in {1, 3}:\n",
    "                warnings.warn(\n",
    "                    'This model usually expects 1 or 3 input channels. '\n",
    "                    'However, it was passed an input_shape with ' +\n",
    "                    str(input_shape[0]) + ' input channels.')\n",
    "            default_shape = (input_shape[0], default_size, default_size)\n",
    "        else:\n",
    "            if input_shape[-1] not in {1, 3}:\n",
    "                warnings.warn(\n",
    "                    'This model usually expects 1 or 3 input channels. '\n",
    "                    'However, it was passed an input_shape with ' +\n",
    "                    str(input_shape[-1]) + ' input channels.')\n",
    "            default_shape = (default_size, default_size, input_shape[-1])\n",
    "    else:\n",
    "        if data_format == 'channels_first':\n",
    "            default_shape = (3, default_size, default_size)\n",
    "        else:\n",
    "            default_shape = (default_size, default_size, 3)\n",
    "    if weights == 'imagenet' and require_flatten:\n",
    "        if input_shape is not None:\n",
    "            if input_shape != default_shape:\n",
    "                raise ValueError('When setting `include_top=True` '\n",
    "                                 'and loading `imagenet` weights, '\n",
    "                                 '`input_shape` should be ' +\n",
    "                                 str(default_shape) + '.')\n",
    "        return default_shape\n",
    "    if input_shape:\n",
    "        if data_format == 'channels_first':\n",
    "            if input_shape is not None:\n",
    "                if len(input_shape) != 3:\n",
    "                    raise ValueError(\n",
    "                        '`input_shape` must be a tuple of three integers.')\n",
    "                if input_shape[0] != 3 and weights == 'imagenet':\n",
    "                    raise ValueError('The input must have 3 channels; got '\n",
    "                                     '`input_shape=' + str(input_shape) + '`')\n",
    "                if ((input_shape[1] is not None and input_shape[1] < min_size) or\n",
    "                   (input_shape[2] is not None and input_shape[2] < min_size)):\n",
    "                    raise ValueError('Input size must be at least ' +\n",
    "                                     str(min_size) + 'x' + str(min_size) +\n",
    "                                     '; got `input_shape=' +\n",
    "                                     str(input_shape) + '`')\n",
    "        else:\n",
    "            if input_shape is not None:\n",
    "                if len(input_shape) != 3:\n",
    "                    raise ValueError(\n",
    "                        '`input_shape` must be a tuple of three integers.')\n",
    "                if input_shape[-1] != 3 and weights == 'imagenet':\n",
    "                    raise ValueError('The input must have 3 channels; got '\n",
    "                                     '`input_shape=' + str(input_shape) + '`')\n",
    "                if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
    "                   (input_shape[1] is not None and input_shape[1] < min_size)):\n",
    "                    raise ValueError('Input size must be at least ' +\n",
    "                                     str(min_size) + 'x' + str(min_size) +\n",
    "                                     '; got `input_shape=' +\n",
    "                                     str(input_shape) + '`')\n",
    "    else:\n",
    "        if require_flatten:\n",
    "            input_shape = default_shape\n",
    "        else:\n",
    "            if data_format == 'channels_first':\n",
    "                input_shape = (3, None, None)\n",
    "            else:\n",
    "                input_shape = (None, None, 3)\n",
    "    if require_flatten:\n",
    "        if None in input_shape:\n",
    "            raise ValueError('If `include_top` is True, '\n",
    "                             'you should specify a static `input_shape`. '\n",
    "                             'Got `input_shape=' + str(input_shape) + '`')\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Augmented Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import concatenate\n",
    "\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _conv_layer(filters, kernel_size, strides=(1, 1), padding='same', name=None):\n",
    "    return Conv2D(filters, kernel_size, strides=strides, padding=padding,\n",
    "                  use_bias=True, kernel_initializer='he_normal', name=name)\n",
    "\n",
    "\n",
    "def _normalize_depth_vars(depth_k, depth_v, filters):\n",
    "    \"\"\"\n",
    "    Accepts depth_k and depth_v as either floats or integers\n",
    "    and normalizes them to integers.\n",
    "    Args:\n",
    "        depth_k: float or int.\n",
    "        depth_v: float or int.\n",
    "        filters: number of output filters.\n",
    "    Returns:\n",
    "        depth_k, depth_v as integers.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(depth_k) == float:\n",
    "        depth_k = int(filters * depth_k)\n",
    "    else:\n",
    "        depth_k = int(depth_k)\n",
    "\n",
    "    if type(depth_v) == float:\n",
    "        depth_v = int(filters * depth_v)\n",
    "    else:\n",
    "        depth_v = int(depth_v)\n",
    "\n",
    "    return depth_k, depth_v\n",
    "\n",
    "\n",
    "class AttentionAugmentation2D(Layer):\n",
    "\n",
    "    def __init__(self, depth_k, depth_v, num_heads, relative=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Applies attention augmentation on a convolutional layer\n",
    "        output.\n",
    "        Args:\n",
    "            depth_k: float or int. Number of filters for k.\n",
    "            Computes the number of filters for `v`.\n",
    "            If passed as float, computed as `filters * depth_k`.\n",
    "        depth_v: float or int. Number of filters for v.\n",
    "            Computes the number of filters for `k`.\n",
    "            If passed as float, computed as `filters * depth_v`.\n",
    "        num_heads: int. Number of attention heads.\n",
    "            Must be set such that `depth_k // num_heads` is > 0.\n",
    "        relative: bool, whether to use relative encodings.\n",
    "        Raises:\n",
    "            ValueError: if depth_v or depth_k is not divisible by\n",
    "                num_heads.\n",
    "        Returns:\n",
    "            Output tensor of shape\n",
    "            -   [Batch, Height, Width, Depth_V] if\n",
    "                channels_last data format.\n",
    "            -   [Batch, Depth_V, Height, Width] if\n",
    "                channels_first data format.\n",
    "        \"\"\"\n",
    "        super(AttentionAugmentation2D, self).__init__(**kwargs)\n",
    "\n",
    "        if depth_k % num_heads != 0:\n",
    "            raise ValueError('`depth_k` (%d) is not divisible by `num_heads` (%d)' % (\n",
    "                depth_k, num_heads))\n",
    "\n",
    "        if depth_v % num_heads != 0:\n",
    "            raise ValueError('`depth_v` (%d) is not divisible by `num_heads` (%d)' % (\n",
    "                depth_v, num_heads))\n",
    "\n",
    "        if depth_k // num_heads < 1.:\n",
    "            raise ValueError('depth_k / num_heads cannot be less than 1 ! '\n",
    "                             'Given depth_k = %d, num_heads = %d' % (\n",
    "                             depth_k, num_heads))\n",
    "\n",
    "        if depth_v // num_heads < 1.:\n",
    "            raise ValueError('depth_v / num_heads cannot be less than 1 ! '\n",
    "                             'Given depth_v = %d, num_heads = %d' % (\n",
    "                                 depth_v, num_heads))\n",
    "\n",
    "        self.depth_k = depth_k\n",
    "        self.depth_v = depth_v\n",
    "        self.num_heads = num_heads\n",
    "        self.relative = relative\n",
    "\n",
    "        self.axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._shape = input_shape\n",
    "\n",
    "        # normalize the format of depth_v and depth_k\n",
    "        self.depth_k, self.depth_v = _normalize_depth_vars(self.depth_k, self.depth_v,\n",
    "                                                           input_shape)\n",
    "\n",
    "        if self.axis == 1:\n",
    "            _, channels, height, width = input_shape\n",
    "        else:\n",
    "            _, height, width, channels = input_shape\n",
    "\n",
    "        if self.relative:\n",
    "            dk_per_head = self.depth_k // self.num_heads\n",
    "\n",
    "            if dk_per_head == 0:\n",
    "                print('dk per head', dk_per_head)\n",
    "\n",
    "            self.key_relative_w = self.add_weight('key_rel_w',\n",
    "                                                  shape=[2 * width - 1, dk_per_head],\n",
    "                                                  initializer=initializers.RandomNormal(\n",
    "                                                      stddev=dk_per_head ** -0.5))\n",
    "\n",
    "            self.key_relative_h = self.add_weight('key_rel_h',\n",
    "                                                  shape=[2 * height - 1, dk_per_head],\n",
    "                                                  initializer=initializers.RandomNormal(\n",
    "                                                      stddev=dk_per_head ** -0.5))\n",
    "\n",
    "        else:\n",
    "            self.key_relative_w = None\n",
    "            self.key_relative_h = None\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.axis == 1:\n",
    "            # If channels first, force it to be channels last for these ops\n",
    "            inputs = K.permute_dimensions(inputs, [0, 2, 3, 1])\n",
    "\n",
    "        q, k, v = tf.split(inputs, [self.depth_k, self.depth_k, self.depth_v], axis=-1)\n",
    "\n",
    "        q = self.split_heads_2d(q)\n",
    "        k = self.split_heads_2d(k)\n",
    "        v = self.split_heads_2d(v)\n",
    "\n",
    "        # scale query\n",
    "        depth_k_heads = self.depth_k / self.num_heads\n",
    "        q *= (depth_k_heads ** -0.5)\n",
    "\n",
    "        # [Batch, num_heads, height * width, depth_k or depth_v] if axis == -1\n",
    "        qk_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_k // self.num_heads]\n",
    "        v_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_v // self.num_heads]\n",
    "        flat_q = K.reshape(q, K.stack(qk_shape))\n",
    "        flat_k = K.reshape(k, K.stack(qk_shape))\n",
    "        flat_v = K.reshape(v, K.stack(v_shape))\n",
    "\n",
    "        # [Batch, num_heads, HW, HW]\n",
    "        logits = tf.matmul(flat_q, flat_k, transpose_b=True)\n",
    "\n",
    "        # Apply relative encodings\n",
    "        if self.relative:\n",
    "            h_rel_logits, w_rel_logits = self.relative_logits(q)\n",
    "            logits += h_rel_logits\n",
    "            logits += w_rel_logits\n",
    "\n",
    "        weights = K.softmax(logits, axis=-1)\n",
    "        attn_out = tf.matmul(weights, flat_v)\n",
    "\n",
    "        attn_out_shape = [self._batch, self.num_heads, self._height, self._width, self.depth_v // self.num_heads]\n",
    "        attn_out_shape = K.stack(attn_out_shape)\n",
    "        attn_out = K.reshape(attn_out, attn_out_shape)\n",
    "        attn_out = self.combine_heads_2d(attn_out)\n",
    "        # [batch, height, width, depth_v]\n",
    "\n",
    "        if self.axis == 1:\n",
    "            # return to [batch, depth_v, height, width] for channels first\n",
    "            attn_out = K.permute_dimensions(attn_out, [0, 3, 1, 2])\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[self.axis] = self.depth_v\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def split_heads_2d(self, ip):\n",
    "        tensor_shape = K.shape(ip)\n",
    "\n",
    "        # batch, height, width, channels for axis = -1\n",
    "        tensor_shape = [tensor_shape[i] for i in range(len(self._shape))]\n",
    "\n",
    "        batch = tensor_shape[0]\n",
    "        height = tensor_shape[1]\n",
    "        width = tensor_shape[2]\n",
    "        channels = tensor_shape[3]\n",
    "\n",
    "        # Save the spatial tensor dimensions\n",
    "        self._batch = batch\n",
    "        self._height = height\n",
    "        self._width = width\n",
    "\n",
    "        ret_shape = K.stack([batch, height, width,  self.num_heads, channels // self.num_heads])\n",
    "        split = K.reshape(ip, ret_shape)\n",
    "        transpose_axes = (0, 3, 1, 2, 4)\n",
    "        split = K.permute_dimensions(split, transpose_axes)\n",
    "\n",
    "        return split\n",
    "\n",
    "    def relative_logits(self, q):\n",
    "        shape = K.shape(q)\n",
    "        # [batch, num_heads, H, W, depth_v]\n",
    "        shape = [shape[i] for i in range(5)]\n",
    "\n",
    "        height = shape[2]\n",
    "        width = shape[3]\n",
    "\n",
    "        rel_logits_w = self.relative_logits_1d(q, self.key_relative_w, height, width,\n",
    "                                               transpose_mask=[0, 1, 2, 4, 3, 5])\n",
    "\n",
    "        rel_logits_h = self.relative_logits_1d(\n",
    "            K.permute_dimensions(q, [0, 1, 3, 2, 4]),\n",
    "            self.key_relative_h, width, height,\n",
    "            transpose_mask=[0, 1, 4, 2, 5, 3])\n",
    "\n",
    "        return rel_logits_h, rel_logits_w\n",
    "\n",
    "    def relative_logits_1d(self, q, rel_k, H, W, transpose_mask):\n",
    "        rel_logits = tf.einsum('bhxyd,md->bhxym', q, rel_k)\n",
    "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads * H, W, 2 * W - 1])\n",
    "        rel_logits = self.rel_to_abs(rel_logits)\n",
    "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H, W, W])\n",
    "        rel_logits = K.expand_dims(rel_logits, axis=3)\n",
    "        rel_logits = K.tile(rel_logits, [1, 1, 1, H, 1, 1])\n",
    "        rel_logits = K.permute_dimensions(rel_logits, transpose_mask)\n",
    "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H * W, H * W])\n",
    "        return rel_logits\n",
    "\n",
    "    def rel_to_abs(self, x):\n",
    "        shape = K.shape(x)\n",
    "        shape = [shape[i] for i in range(3)]\n",
    "        B, Nh, L, = shape\n",
    "        col_pad = K.zeros(K.stack([B, Nh, L, 1]))\n",
    "        x = K.concatenate([x, col_pad], axis=3)\n",
    "        flat_x = K.reshape(x, [B, Nh, L * 2 * L])\n",
    "        flat_pad = K.zeros(K.stack([B, Nh, L - 1]))\n",
    "        flat_x_padded = K.concatenate([flat_x, flat_pad], axis=2)\n",
    "        final_x = K.reshape(flat_x_padded, [B, Nh, L + 1, 2 * L - 1])\n",
    "        final_x = final_x[:, :, :L, L - 1:]\n",
    "        return final_x\n",
    "\n",
    "    def combine_heads_2d(self, inputs):\n",
    "        # [batch, num_heads, height, width, depth_v // num_heads]\n",
    "        transposed = K.permute_dimensions(inputs, [0, 2, 3, 1, 4])\n",
    "        # [batch, height, width, num_heads, depth_v // num_heads]\n",
    "        shape = K.shape(transposed)\n",
    "        shape = [shape[i] for i in range(5)]\n",
    "\n",
    "        a, b = shape[-2:]\n",
    "        ret_shape = K.stack(shape[:-2] + [a * b])\n",
    "        # [batch, height, width, depth_v]\n",
    "        return K.reshape(transposed, ret_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'depth_k': self.depth_k,\n",
    "            'depth_v': self.depth_v,\n",
    "            'num_heads': self.num_heads,\n",
    "            'relative': self.relative,\n",
    "        }\n",
    "        base_config = super(AttentionAugmentation2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def augmented_conv2d(ip, filters, kernel_size=(3, 3), strides=(1, 1),\n",
    "                     depth_k=0.2, depth_v=0.2, num_heads=8, relative_encodings=True):\n",
    "    \"\"\"\n",
    "    Builds an Attention Augmented Convolution block.\n",
    "    Args:\n",
    "        ip: keras tensor.\n",
    "        filters: number of output filters.\n",
    "        kernel_size: convolution kernel size.\n",
    "        strides: strides of the convolution.\n",
    "        depth_k: float or int. Number of filters for k.\n",
    "            Computes the number of filters for `v`.\n",
    "            If passed as float, computed as `filters * depth_k`.\n",
    "        depth_v: float or int. Number of filters for v.\n",
    "            Computes the number of filters for `k`.\n",
    "            If passed as float, computed as `filters * depth_v`.\n",
    "        num_heads: int. Number of attention heads.\n",
    "            Must be set such that `depth_k // num_heads` is > 0.\n",
    "        relative_encodings: bool. Whether to use relative\n",
    "            encodings or not.\n",
    "    Returns:\n",
    "        a keras tensor.\n",
    "    \"\"\"\n",
    "    print(f\"Using {num_heads} number of heads\")\n",
    "    # input_shape = K.int_shape(ip)\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    depth_k, depth_v = _normalize_depth_vars(depth_k, depth_v, filters)\n",
    "\n",
    "    conv_out = _conv_layer(filters - depth_v, kernel_size, strides)(ip)\n",
    "\n",
    "    # Augmented Attention Block\n",
    "    qkv_conv = _conv_layer(2 * depth_k + depth_v, (1, 1), strides)(ip)\n",
    "    attn_out = AttentionAugmentation2D(depth_k, depth_v, num_heads, relative_encodings)(qkv_conv)\n",
    "    attn_out = _conv_layer(depth_v, kernel_size=(1, 1))(attn_out)\n",
    "\n",
    "    output = concatenate([conv_out, attn_out], axis=channel_axis)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_VGG16(include_top=False, # switched from True to False\n",
    "          weights='imagenet',\n",
    "          input_tensor=None,\n",
    "          input_shape=None,\n",
    "          pooling=None,\n",
    "          classes=2, # switched from 1000 default classes to 2 classes\n",
    "          **kwargs):\n",
    "    \"\"\"Instantiates the VGG16 architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    # Arguments\n",
    "        include_top: whether to include the 3 fully-connected\n",
    "            layers at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)`\n",
    "            (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 input channels,\n",
    "            and width and height should be no smaller than 32.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=32,\n",
    "                                      data_format=backend.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    # Multi-Headed Attention Layer\n",
    "    x = augmented_conv2d(img_input)\n",
    "            \n",
    "    # Block 1\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv1')(x)\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv2')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1')(x)\n",
    "    x = layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1')(x)\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv2')(x)\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv3')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv1')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv2')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv3')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv1')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv2')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv3')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = layers.Flatten(name='flatten')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='vgg16')\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                file_hash='64373286793e3c8b2b4e3219cbf3544b')\n",
    "        else:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                file_hash='6d6bbae143d832006294945121d1f1fc')\n",
    "        model.load_weights(weights_path)\n",
    "        if backend.backend() == 'theano':\n",
    "            keras_utils.convert_all_kernels_in_model(model)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 number of heads\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_32 (InputLayer)           (None, 20, 26, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 20, 26, 12)   48          input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_augmentation2d_31 (At (None, 20, 26, 4)    180         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 20, 26, 16)   448         input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 20, 26, 4)    20          attention_augmentation2d_31[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 20, 26, 20)   0           conv2d_82[0][0]                  \n",
      "                                                                 conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "final_block_pool (MaxPooling2D) (None, 10, 13, 20)   0           concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 2600)         0           final_block_pool[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 128)          332928      flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 2)            258         dense_37[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 333,882\n",
      "Trainable params: 333,882\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dims = (WIDTH, HEIGHT, 3)\n",
    "inputs = Input(shape=input_dims)\n",
    "x = augmented_conv2d(inputs, filters=20, kernel_size=(3, 3), depth_k=0.2, depth_v=0.2,  # dk/v (0.2) * f_out (20) = 4\n",
    "                         num_heads=2, relative_encodings=True)\n",
    "x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='final_block_pool')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=Adam(lr=0.000001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding shape from (1678, 20, 26, 3) to (1678, 20, 26, 3, 1)\n",
      "Expanding shape from (718, 20, 26, 3) to (718, 20, 26, 3, 1)\n",
      "Expanding shape from (2396, 20, 26, 3) to (2396, 20, 26, 3, 1)\n",
      "The shape of X_train_expanded is (1678, 20, 26, 3, 1)\n",
      "The shape of X_test_expanded is (718, 20, 26, 3, 1)\n",
      "The shape of X_train is (1678, 20, 26, 3)\n",
      "The shape of y_train is (1678, 2)\n",
      "The shape of X_test is (718, 20, 26, 3)\n",
      "The shape of y_test is (718, 2) - some example targets:\n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "images: np.ndarray = np.array(images)\n",
    "X_train = images[training_indices]\n",
    "y_train = binary_target[training_indices]\n",
    "y_train = encoded_target[training_indices]\n",
    "X_test = images[validation_indices]\n",
    "y_test = binary_target[validation_indices]\n",
    "y_test = encoded_target[validation_indices]\n",
    "X_train_expanded: np.ndarray = expand_tensor_shape(X_train)\n",
    "X_test_expanded: np.ndarray = expand_tensor_shape(X_test)\n",
    "images_expanded = expand_tensor_shape(images)\n",
    "\n",
    "print(f\"The shape of X_train_expanded is {X_train_expanded.shape}\")\n",
    "print(f\"The shape of X_test_expanded is {X_test_expanded.shape}\")\n",
    "print(f\"The shape of X_train is {X_train.shape}\")\n",
    "print(f\"The shape of y_train is {y_train.shape}\")\n",
    "print(f\"The shape of X_test is {X_test.shape}\")\n",
    "print(f\"The shape of y_test is {y_test.shape} - some example targets:\\n {y_test[:5]}\")\n",
    "\n",
    "input_dims = (WIDTH, HEIGHT, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1678/1678 [==============================] - 5s 3ms/step - loss: 8.0806 - acc: 0.4982\n",
      "Epoch 2/20\n",
      "1678/1678 [==============================] - 3s 2ms/step - loss: 8.0751 - acc: 0.4976\n",
      "Epoch 3/20\n",
      "1678/1678 [==============================] - 3s 2ms/step - loss: 8.0879 - acc: 0.4970\n",
      "Epoch 4/20\n",
      "1678/1678 [==============================] - 3s 2ms/step - loss: 8.0493 - acc: 0.4988\n",
      "Epoch 5/20\n",
      "1678/1678 [==============================] - 3s 2ms/step - loss: 8.0413 - acc: 0.4988\n",
      "Epoch 6/20\n",
      "1678/1678 [==============================] - 3s 2ms/step - loss: 8.0792 - acc: 0.4976\n",
      "Epoch 7/20\n",
      "1678/1678 [==============================] - 3s 2ms/step - loss: 8.0417 - acc: 0.5000\n",
      "Epoch 8/20\n",
      "1678/1678 [==============================] - 3s 2ms/step - loss: 8.0273 - acc: 0.5012\n",
      "Epoch 9/20\n",
      "1678/1678 [==============================] - 3s 2ms/step - loss: 8.0245 - acc: 0.5012\n",
      "Epoch 10/20\n",
      "1678/1678 [==============================] - 3s 2ms/step - loss: 8.0194 - acc: 0.5018\n",
      "Epoch 11/20\n",
      "1678/1678 [==============================] - 3s 2ms/step - loss: 8.0310 - acc: 0.5012\n",
      "Epoch 12/20\n",
      "1600/1678 [===========================>..] - ETA: 0s - loss: 8.0526 - acc: 0.4988"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-c2a335988c89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
