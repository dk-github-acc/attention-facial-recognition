{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "ImageShape = Tuple[int, int]\n",
    "GrayScaleImageShape = Tuple[int, int, int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Sandbox Baseline Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sandbox example is meant mostly to establish a few baselines for model performance to compare against, and also to get the basic Keras neural network architecture set up. I split the training and testing data and then one-hot encode the targets (one column per target, so ten columns after encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is (60000, 28, 28)\n",
      "The shape of y_train is (60000,)\n",
      "The shape of X_test is (10000, 28, 28)\n",
      "The shape of y_test is (10000,) - some example targets: [7 2 1 0 4]\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "Dataset = Tuple[np.ndarray, np.ndarray]\n",
    "\n",
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(f\"The shape of X_train is {X_train.shape}\")\n",
    "print(f\"The shape of y_train is {y_train.shape}\")\n",
    "print(f\"The shape of X_test is {X_test.shape}\")\n",
    "print(f\"The shape of y_test is {y_test.shape} - some example targets: {y_test[:5]}\")\n",
    "mnist_image_shape: ImageShape = X_train.shape[1:]\n",
    "print(mnist_image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding y_train (60000,) -> (60000, 10)\n",
      "One-hot encoding y_test (10000,) -> (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "OneHotEncodedTarget = np.ndarray\n",
    "Categories = int\n",
    "encoded_y_train: OneHotEncodedTarget = to_categorical(y_train)\n",
    "encoded_y_test: OneHotEncodedTarget = to_categorical(y_test)\n",
    "print(f\"One-hot encoding y_train {y_train.shape} -> {encoded_y_train.shape}\")\n",
    "print(f\"One-hot encoding y_test {y_test.shape} -> {encoded_y_test.shape}\")\n",
    "\n",
    "K: Categories = encoded_y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla CNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vanilla CNN implementation, with two convolutional layers, 64 and 32 filters each, with kernel size of `3 x 3`. Then the values are flattened and fed into the final softmax classification dense layer for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_3/Softmax:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# define model architecture and hyperparameters\n",
    "NUM_FILTERS_L1 = 64\n",
    "NUM_FILTERS_L2 = 32\n",
    "KERNEL_SIZE = 3\n",
    "\n",
    "# the images are 28 x 28 (pixel size) x 1 (grayscale - if RGB, then 3)\n",
    "input_dims: GrayScaleImageShape = (28,28,1)\n",
    "\n",
    "def build_vanilla_cnn(filters_layer1:int, filters_layer2:int, kernel_size:int, input_dims: GrayScaleImageShape)-> Model:\n",
    "    inputs: Tensor = Input(shape=input_dims)\n",
    "    x: Tensor = Conv2D(filters=filters_layer1, kernel_size=kernel_size, activation='relu')(inputs)\n",
    "    x: Tensor = Conv2D(filters=filters_layer2, kernel_size=kernel_size, activation='relu')(x)\n",
    "    x: Tensor = Flatten()(x)\n",
    "    predictions = Dense(K, activation=\"softmax\")(x)\n",
    "    print(predictions)\n",
    "\n",
    "    #compile model using accuracy to measure model performance\n",
    "    model: Model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model: Model = build_vanilla_cnn(NUM_FILTERS_L1, NUM_FILTERS_L2, KERNEL_SIZE, input_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function to Expand Tensor Dimensions By 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding shape from (60000, 28, 28) to (60000, 28, 28, 1)\n",
      "Expanding shape from (10000, 28, 28) to (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train.reshape((60000,1,28,28))\n",
    "\n",
    "def expand_tensor_shape(X_train: np.ndarray)-> np.ndarray:\n",
    "    new_shape: Tuple = X_train.shape + (1,)\n",
    "        \n",
    "#     new_tensor = X_train.reshape(new_shape).reshape((-1,1,28,28))\n",
    "    new_tensor = X_train.reshape(new_shape)\n",
    "    print(f\"Expanding shape from {X_train.shape} to {new_tensor.shape}\")\n",
    "    return new_tensor\n",
    "\n",
    "X_train_expanded: np.ndarray = expand_tensor_shape(X_train)\n",
    "X_test_expanded: np.ndarray = expand_tensor_shape(X_test)\n",
    "    \n",
    "    \n",
    "# train model and retrieve history\n",
    "# from keras.callbacks import History\n",
    "# history: History = model.fit(X_train_expanded, encoded_y_train, \n",
    "#                              validation_data=(X_test_expanded, encoded_y_test), epochs=2, batch_size=2058)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Average Pooling Layer\n",
    "\n",
    "Output shape of convolutional layer is typically `batch size x number of filters x width x height`. The GAP layer will take the average of the width/height axis and return a vector of length equal to the number of filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(X_train_expanded, (-1,1,28,28)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, Flatten, Input, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,162.0\n",
      "Trainable params: 1,162.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 5s - loss: 6.9768 - acc: 0.0942 - val_loss: 4.3748 - val_acc: 0.0978\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 5s - loss: 3.3634 - acc: 0.0858 - val_loss: 2.5592 - val_acc: 0.1437\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 6s - loss: 2.5739 - acc: 0.0908 - val_loss: 2.3753 - val_acc: 0.0980\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 6s - loss: 2.3633 - acc: 0.1137 - val_loss: 2.2549 - val_acc: 0.1373\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 6s - loss: 2.2353 - acc: 0.1205 - val_loss: 2.1695 - val_acc: 0.1765\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 6s - loss: 2.1458 - acc: 0.1797 - val_loss: 2.0833 - val_acc: 0.1997\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 7s - loss: 2.0661 - acc: 0.2243 - val_loss: 2.0146 - val_acc: 0.2675\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.9997 - acc: 0.2866 - val_loss: 1.9549 - val_acc: 0.3238\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.9400 - acc: 0.3411 - val_loss: 1.8985 - val_acc: 0.3675\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.8843 - acc: 0.3772 - val_loss: 1.8459 - val_acc: 0.4216\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.8285 - acc: 0.4098 - val_loss: 1.7896 - val_acc: 0.4348\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.7714 - acc: 0.4373 - val_loss: 1.7330 - val_acc: 0.4615\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 6s - loss: 1.7167 - acc: 0.4590 - val_loss: 1.6803 - val_acc: 0.4677\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.6663 - acc: 0.4762 - val_loss: 1.6348 - val_acc: 0.4737\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.6211 - acc: 0.4913 - val_loss: 1.5894 - val_acc: 0.5145\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 6s - loss: 1.5786 - acc: 0.5081 - val_loss: 1.5476 - val_acc: 0.5200\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.5394 - acc: 0.5226 - val_loss: 1.5062 - val_acc: 0.5409\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.5026 - acc: 0.5362 - val_loss: 1.4688 - val_acc: 0.5518\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.4656 - acc: 0.5511 - val_loss: 1.4327 - val_acc: 0.5611\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.4324 - acc: 0.5623 - val_loss: 1.3981 - val_acc: 0.5679\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.3992 - acc: 0.5735 - val_loss: 1.3634 - val_acc: 0.5926\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.3662 - acc: 0.5861 - val_loss: 1.3278 - val_acc: 0.6079\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.3357 - acc: 0.5987 - val_loss: 1.2963 - val_acc: 0.6019\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.3060 - acc: 0.6063 - val_loss: 1.2654 - val_acc: 0.6233\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.2761 - acc: 0.6216 - val_loss: 1.2373 - val_acc: 0.6224\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.2490 - acc: 0.6269 - val_loss: 1.2076 - val_acc: 0.6373\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 6s - loss: 1.2217 - acc: 0.6381 - val_loss: 1.1771 - val_acc: 0.6549\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.1946 - acc: 0.6498 - val_loss: 1.1507 - val_acc: 0.6625\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.1705 - acc: 0.6570 - val_loss: 1.1220 - val_acc: 0.6878\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 6s - loss: 1.1457 - acc: 0.6672 - val_loss: 1.0987 - val_acc: 0.6810\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.1228 - acc: 0.6702 - val_loss: 1.0761 - val_acc: 0.7052\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.1012 - acc: 0.6811 - val_loss: 1.0534 - val_acc: 0.6992\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.0790 - acc: 0.6898 - val_loss: 1.0358 - val_acc: 0.6848\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.0602 - acc: 0.6938 - val_loss: 1.0097 - val_acc: 0.7075\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.0395 - acc: 0.7027 - val_loss: 0.9923 - val_acc: 0.7181\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 6s - loss: 1.0215 - acc: 0.7089 - val_loss: 0.9722 - val_acc: 0.7313\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 7s - loss: 1.0040 - acc: 0.7160 - val_loss: 0.9545 - val_acc: 0.7237\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.9888 - acc: 0.7174 - val_loss: 0.9346 - val_acc: 0.7467\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.9706 - acc: 0.7267 - val_loss: 0.9182 - val_acc: 0.7474\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.9554 - acc: 0.7314 - val_loss: 0.9063 - val_acc: 0.7419\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.9402 - acc: 0.7340 - val_loss: 0.8900 - val_acc: 0.7597\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.9273 - acc: 0.7399 - val_loss: 0.8740 - val_acc: 0.7593\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.9132 - acc: 0.7449 - val_loss: 0.8614 - val_acc: 0.7590\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.9001 - acc: 0.7478 - val_loss: 0.8468 - val_acc: 0.7638\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.8875 - acc: 0.7530 - val_loss: 0.8359 - val_acc: 0.7683\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.8764 - acc: 0.7545 - val_loss: 0.8251 - val_acc: 0.7746\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.8659 - acc: 0.7582 - val_loss: 0.8163 - val_acc: 0.7788\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.8557 - acc: 0.7617 - val_loss: 0.8051 - val_acc: 0.7811\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.8462 - acc: 0.7627 - val_loss: 0.7947 - val_acc: 0.7791\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.8383 - acc: 0.7662 - val_loss: 0.7873 - val_acc: 0.7821\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.8262 - acc: 0.7707 - val_loss: 0.7774 - val_acc: 0.7849\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.8174 - acc: 0.7722 - val_loss: 0.7658 - val_acc: 0.7945\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.8089 - acc: 0.7752 - val_loss: 0.7568 - val_acc: 0.7924\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.7997 - acc: 0.7789 - val_loss: 0.7467 - val_acc: 0.7988\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.7913 - acc: 0.7809 - val_loss: 0.7396 - val_acc: 0.8003\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.7842 - acc: 0.7834 - val_loss: 0.7320 - val_acc: 0.7993\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.7758 - acc: 0.7841 - val_loss: 0.7230 - val_acc: 0.8034\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.7696 - acc: 0.7849 - val_loss: 0.7161 - val_acc: 0.8054\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.7612 - acc: 0.7893 - val_loss: 0.7125 - val_acc: 0.8020\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.7543 - acc: 0.7921 - val_loss: 0.7062 - val_acc: 0.8066\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.7487 - acc: 0.7917 - val_loss: 0.6959 - val_acc: 0.8111\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.7413 - acc: 0.7949 - val_loss: 0.6893 - val_acc: 0.8127\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.7340 - acc: 0.7984 - val_loss: 0.6845 - val_acc: 0.8123\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.7280 - acc: 0.7985 - val_loss: 0.6766 - val_acc: 0.8143\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.7225 - acc: 0.7997 - val_loss: 0.6694 - val_acc: 0.8178\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.7170 - acc: 0.8009 - val_loss: 0.6652 - val_acc: 0.8184\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.7120 - acc: 0.8022 - val_loss: 0.6617 - val_acc: 0.8182\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.7068 - acc: 0.8032 - val_loss: 0.6553 - val_acc: 0.8237\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.7014 - acc: 0.8053 - val_loss: 0.6489 - val_acc: 0.8224\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.6937 - acc: 0.8083 - val_loss: 0.6426 - val_acc: 0.8230\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.6879 - acc: 0.8100 - val_loss: 0.6414 - val_acc: 0.8223\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6869 - acc: 0.8084 - val_loss: 0.6366 - val_acc: 0.8258\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.6800 - acc: 0.8115 - val_loss: 0.6277 - val_acc: 0.8274\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.6751 - acc: 0.8137 - val_loss: 0.6249 - val_acc: 0.8278\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6702 - acc: 0.8148 - val_loss: 0.6212 - val_acc: 0.8285\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.6669 - acc: 0.8151 - val_loss: 0.6162 - val_acc: 0.8301\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6610 - acc: 0.8176 - val_loss: 0.6136 - val_acc: 0.8268\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6565 - acc: 0.8184 - val_loss: 0.6097 - val_acc: 0.8290\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6518 - acc: 0.8195 - val_loss: 0.6002 - val_acc: 0.8342\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6455 - acc: 0.8223 - val_loss: 0.5985 - val_acc: 0.8351\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6423 - acc: 0.8238 - val_loss: 0.5986 - val_acc: 0.8324\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 8s - loss: 0.6404 - acc: 0.8232 - val_loss: 0.5925 - val_acc: 0.8345\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6359 - acc: 0.8245 - val_loss: 0.5882 - val_acc: 0.8350\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6314 - acc: 0.8262 - val_loss: 0.5828 - val_acc: 0.8401\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6274 - acc: 0.8269 - val_loss: 0.5807 - val_acc: 0.8410\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6240 - acc: 0.8278 - val_loss: 0.5744 - val_acc: 0.8419\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6190 - acc: 0.8294 - val_loss: 0.5707 - val_acc: 0.8446\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6166 - acc: 0.8304 - val_loss: 0.5678 - val_acc: 0.8433\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6120 - acc: 0.8313 - val_loss: 0.5652 - val_acc: 0.8433\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6082 - acc: 0.8322 - val_loss: 0.5585 - val_acc: 0.8501\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 6s - loss: 0.6042 - acc: 0.8342 - val_loss: 0.5562 - val_acc: 0.8522\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.6035 - acc: 0.8336 - val_loss: 0.5552 - val_acc: 0.8471\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.5998 - acc: 0.8350 - val_loss: 0.5585 - val_acc: 0.8478\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.5968 - acc: 0.8343 - val_loss: 0.5460 - val_acc: 0.8515\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.5894 - acc: 0.8379 - val_loss: 0.5444 - val_acc: 0.8518\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 8s - loss: 0.5874 - acc: 0.8382 - val_loss: 0.5442 - val_acc: 0.8492\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.5847 - acc: 0.8388 - val_loss: 0.5379 - val_acc: 0.8563\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.5823 - acc: 0.8391 - val_loss: 0.5384 - val_acc: 0.8563\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.5815 - acc: 0.8391 - val_loss: 0.5382 - val_acc: 0.8512\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 7s - loss: 0.5796 - acc: 0.8395 - val_loss: 0.5268 - val_acc: 0.8584\n"
     ]
    }
   ],
   "source": [
    "###### from keras.layers import Layer, Lambda, Input\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, MaxPool2D\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "def global_average_pooling(x: Layer):\n",
    "    return K.mean(x, axis = (2,3))\n",
    "\n",
    "def global_average_pooling_shape(input_shape):\n",
    "    # return the dimensions corresponding with batch size and number of filters\n",
    "    return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def build_global_average_pooling_layer(function, output_shape):\n",
    "    return Lambda(pooling_function, output_shape)\n",
    "\n",
    "inputs: Tensor = Input(shape=(28,28,1))\n",
    "x: Tensor = Conv2D(filters=32, kernel_size=5, activation='relu')(inputs)\n",
    "# x: Tensor = MaxPool2D()(x)\n",
    "# x: Tensor = Conv2D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "x: Tensor = Lambda(lambda x: K.mean(x, axis=(1,2)), output_shape=global_average_pooling_shape)(x)\n",
    "# x: Tensor = Dense(128, activation=\"relu\")(x)\n",
    "predictions: Tensor = Dense(10, activation=\"softmax\")(x)\n",
    "model: Model = Model(inputs=inputs, outputs=predictions)\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "from keras.callbacks import History\n",
    "history: History = model.fit(X_train_expanded, encoded_y_train,  \n",
    "                             validation_data=(X_test_expanded, encoded_y_test), epochs=100, batch_size=5126)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Class Activation Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from keras.layers import Layer, Lambda\n",
    "def global_average_pooling(x: Layer):\n",
    "    return K.mean(x, axis = (2,3))\n",
    "\n",
    "def global_average_pooling_shape(input_shape):\n",
    "    # return only the first two dimensions (batch size and number of filters)\n",
    "    return input_shape[0:2]\n",
    "\n",
    "def build_global_average_pooling_layer(function, output_shape):\n",
    "    return Lambda(pooling_function, output_shape)\n",
    "\n",
    "\n",
    "def get_output_layer(model, layer_name):\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "    layer = layer_dict[layer_name]\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist mode\n",
    "\n",
    "save_filepath: str = \"basic_cam.h5\"\n",
    "model.save(save_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2115519bfd0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAO0ElEQVR4nO3de4xU53nH8d/DsgaHhIbrdg00hIDlGCNDu4bWthJcN5FjpcaJmzioibBqlVSFNLFQU1+k2FGlilaNXTvNpbgmJk6CG/kS08SKgxARjZxaLARzKeYSgvEaArGxDBgDu8vTP/YQbfCed5Y5M3PGPN+PNJqZ88yZ8zDw48zMO+e85u4CcP4bUnYDABqDsANBEHYgCMIOBEHYgSCGNnJjF9gwH64RjdwkEMoJvaFTftIGqhUKu5ldJ+l+SS2S/tPdl6YeP1wjNMeuLbJJAAnP+ZrcWtVv482sRdLXJH1E0qWS5pvZpdU+H4D6KvKZfbak3e6+x91PSXpU0rzatAWg1oqEfYKkl/rd78qW/Q4zW2hmnWbW2a2TBTYHoIgiYR/oS4C3/PbW3Ze5e4e7d7RqWIHNASiiSNi7JE3qd3+ipP3F2gFQL0XCvl7SNDN7r5ldIOlTklbVpi0AtVb10Ju795jZYknPqG/obbm7b6tZZwBqqtA4u7s/LenpGvUCoI74uSwQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBFJrFFWgZMzpZt98bmVvbd9NFyXVPjPVkfeqXn0/WTx8/nqxHUyjsZrZX0lFJvZJ63L2jFk0BqL1a7NmvcfdXavA8AOqIz+xAEEXD7pJ+YmYbzGzhQA8ws4Vm1mlmnd06WXBzAKpV9G38Ve6+38zGS1ptZi+4+7r+D3D3ZZKWSdJIG53+xgVA3RTas7v7/uz6kKQnJc2uRVMAaq/qsJvZCDN715nbkj4saWutGgNQW0XexrdJetLMzjzP99z9xzXpCg0z5LJLkvVdd1yYrP/VjGeT9SVjnjnnngbr/W1/k6xPu2VD3bb9dlR12N19j6TLa9gLgDpi6A0IgrADQRB2IAjCDgRB2IEgOMT1PGBXzMit7b6tJbnuT6/+92R9XMuwZH1Ihf3Fj46Pyq3tOTk+ue6iUTuS9Uc+8GCy/o9XLMit+fotyXXPR+zZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtmbQMu4ccn6zvsnJOv/feXXc2tTWlsrbD09jl7Jt45MStZ/cNPVubXTw9K9Lfphepy9Y1hvsv5mW/7hucOTa56f2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMszeBlz89LVnf9sH7KzxDpbH06n2n0jj6jVcm6707dubWbNb0qnpCddizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLM3gQk37K3bcz927PeT9Xt3Xpust33Rk/XeHbvOuaczXpsxsup1ce4q7tnNbLmZHTKzrf2WjTaz1Wa2K7vOnwkAQFMYzNv4hyVdd9ay2yWtcfdpktZk9wE0sYphd/d1kg6ftXiepBXZ7RWSbqxxXwBqrNov6Nrc/YAkZde5k3aZ2UIz6zSzzm6drHJzAIqq+7fx7r7M3TvcvaO14MkNAVSv2rAfNLN2ScquD9WuJQD1UG3YV0k6Mx/uAklP1aYdAPVScZzdzFZKmitprJl1Sbpb0lJJ3zezWyXtk/SJejZ53vvr9MebSxd9LlmftDr//Okjtv06ue7YF/OPN5ek9JnZizneZnV8dpytYtjdfX5OKf1rDABNhZ/LAkEQdiAIwg4EQdiBIAg7EASHuDaB3t2/Stan3paup/RUvWb9dV9xtOwWQmHPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM4e3L4vpadc7nlH+lTSqnSUamL1j0/7eYWV0xZ3zU3WL/zxxtxahT/VeYk9OxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTj720DLyPTUxidmT8uttd5xMLnu5ku+WlVPv31+a0nWu736k1GvffMdyXrXwj9I1r1ne9XbPh+xZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnbwAblp6S+dQHZyTrt339kWT9mgvX5NYO9p5Mrrv2zVHJ+pd2zkvWV05/OFm/aGj6z54yfEh3sr7nk+9O1qfsGJ5bO33iRFU9vZ1V3LOb2XIzO2RmW/stu8fMXjazTdnl+vq2CaCowbyNf1jSdQMsv8/dZ2aXp2vbFoBaqxh2d18n6XADegFQR0W+oFtsZpuzt/m5H/zMbKGZdZpZZ7fSnx8B1E+1Yf+GpPdJminpgKSv5D3Q3Ze5e4e7d7Sq+i9rABRTVdjd/aC797r7aUkPSppd27YA1FpVYTez9n53PyZpa95jATSHiuPsZrZS0lxJY82sS9Ldkuaa2Uz1nX57r6TP1rHHpjdkeP54riS9evOsZP1//umBQtufvvJzubWJa9PHkw/70fpkfUz7sWR95TN/lKwvGVP9fmDOsPQ4++Zb0q/bn7z0d7m1tm8/n1z39PHjyfrbUcWwu/v8ARY/VIdeANQRP5cFgiDsQBCEHQiCsANBEHYgCHNv3OS1I220z7FrG7a9WkodprrjvsuT674w72uFtj1vx43J+pD5+UNUvQcPJdcdOmlisn75qn3J+pfH/yJZf/10/qGkcx5fkly3/ZJ072tm/FeynnLz7o8m6688MDlZH/5qeliwkpaf5k8nXcRzvkZH/PCAE2mzZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIDiVdMaGpl+KHf+WP5b+wg3pcfSunvTpuG74jy8m65OX/zJZ70mMpXf/WfoQ1Mv+OT1Ofvf4Dcn6t468J1l/5K4/z61NfeJ/k+u2jB2TrM/9UP6hvZL0xs2v59aenPVgct2JDxQ7q9IP30j3vuziKYWevxrs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCI5nz3TdcWWyvnHx/bm1/RXG0W9a+vfJevsPfpWsH75mcrLun34lt/bYZQ8n1x3Xkh5Pnv5oeiz74mX525ak3h27k/WyHPrb9N9321+8WGwDS9LTSfsvthV7/hwczw6AsANREHYgCMIOBEHYgSAIOxAEYQeCYJw9c9eeTcl6avrgw73pcfZvvjYnWZ9wwWvJ+oKRBcd8E6Z/L39aY0maekd6Smfv6allOyio0Di7mU0ys7Vmtt3MtpnZ57Plo81stZntyq5H1bpxALUzmLfxPZKWuPv7Jf2xpEVmdqmk2yWtcfdpktZk9wE0qYphd/cD7r4xu31U0nZJEyTNk7Qie9gKSek5igCU6py+oDOzyZJmSXpOUpu7H5D6/kOQND5nnYVm1mlmnd1Kf7YFUD+DDruZvVPS45K+4O5HBrueuy9z9w5372hVsZP4AajeoMJuZq3qC/p33f2JbPFBM2vP6u2S0lNuAihVxVNJm5lJekjSdne/t19plaQFkpZm10/VpcMGWXfskmR9zrAtubXRFQ4TvXNselivko++8PFkfd/P86ddnvJY/umUJWnqtvSpohlaO38M5rzxV0n6jKQtZnbmX+2d6gv5983sVkn7JH2iPi0CqIWKYXf3n0kacJBeUnP+QgbAW/BzWSAIwg4EQdiBIAg7EARhB4JgyubMs9dclKzP+cs/za29fvmp5LpDf9OarF/8zZfT6/86/XulySdeyq2dTq6JSNizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLNnel89nKy3PfBsfq3gtjliHI3Anh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCqBh2M5tkZmvNbLuZbTOzz2fL7zGzl81sU3a5vv7tAqjWYE5e0SNpibtvNLN3SdpgZquz2n3u/q/1aw9ArQxmfvYDkg5kt4+a2XZJE+rdGIDaOqfP7GY2WdIsSc9lixab2WYzW25mo3LWWWhmnWbW2a2ThZoFUL1Bh93M3inpcUlfcPcjkr4h6X2SZqpvz/+VgdZz92Xu3uHuHa0aVoOWAVRjUGE3s1b1Bf277v6EJLn7QXfvdffTkh6UNLt+bQIoajDfxpukhyRtd/d7+y1v7/ewj0naWvv2ANTKYL6Nv0rSZyRtMbNN2bI7Jc03s5mSXNJeSZ+tS4cAamIw38b/TJINUHq69u0AqBd+QQcEQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQjC3L1xGzP7jaQX+y0aK+mVhjVwbpq1t2btS6K3atWyt/e4+7iBCg0N+1s2btbp7h2lNZDQrL01a18SvVWrUb3xNh4IgrADQZQd9mUlbz+lWXtr1r4keqtWQ3or9TM7gMYpe88OoEEIOxBEKWE3s+vMbIeZ7Taz28voIY+Z7TWzLdk01J0l97LczA6Z2dZ+y0ab2Woz25VdDzjHXkm9NcU03olpxkt97cqe/rzhn9nNrEXSTkkfktQlab2k+e7+fw1tJIeZ7ZXU4e6l/wDDzD4g6Zikb7v7Zdmyf5F02N2XZv9RjnL3f2iS3u6RdKzsabyz2Yra+08zLulGSbeoxNcu0dcn1YDXrYw9+2xJu919j7ufkvSopHkl9NH03H2dpMNnLZ4naUV2e4X6/rE0XE5vTcHdD7j7xuz2UUlnphkv9bVL9NUQZYR9gqSX+t3vUnPN9+6SfmJmG8xsYdnNDKDN3Q9Iff94JI0vuZ+zVZzGu5HOmma8aV67aqY/L6qMsA80lVQzjf9d5e5/KOkjkhZlb1cxOIOaxrtRBphmvClUO/15UWWEvUvSpH73J0raX0IfA3L3/dn1IUlPqvmmoj54Zgbd7PpQyf38VjNN4z3QNONqgteuzOnPywj7eknTzOy9ZnaBpE9JWlVCH29hZiOyL05kZiMkfVjNNxX1KkkLstsLJD1VYi+/o1mm8c6bZlwlv3alT3/u7g2/SLpefd/I/1LSXWX0kNPXFEnPZ5dtZfcmaaX63tZ1q+8d0a2SxkhaI2lXdj26iXp7RNIWSZvVF6z2knq7Wn0fDTdL2pRdri/7tUv01ZDXjZ/LAkHwCzogCMIOBEHYgSAIOxAEYQeCIOxAEIQdCOL/Ab+hZHhXLzvmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_image = X_train[5]\n",
    "first_image = first_image.reshape(28,28,1)\n",
    "img = np.array(first_image).reshape(1, 28, 28, 1)\n",
    "img.shape\n",
    "plt.imshow(img.reshape((28,28)))\n",
    "#img = np.array([np.transpose(np.float32(first_image), (2, 0, 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Basic Model\n",
    "(since the model files are so large, they cannot be pushed to Github- just email me for a copy of the `.h5` model files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"basic_cam.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_10_layer: Layer = model.layers[-1]\n",
    "dense_10_weights = dense_10_layer.get_weights()[0]\n",
    "print(f\"Dense 10 weights: {dense_10_weights.shape}\")\n",
    "dense_128_layer: Layer = model.layers[-2]\n",
    "dense_128_weights = dense_128_layer.get_weights()[0]\n",
    "print(f\"Dense 128 weights: {dense_128_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the Final Class Activation Map Back to the Original Input Shapes and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 24, 32)\n",
      "(32, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'True Image')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "class_weights = model.layers[-1].get_weights()[0]\n",
    "final_conv_layer = get_output_layer(model, \"conv2d_1\")\n",
    "\n",
    "get_output = K.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n",
    "[conv_outputs, predictions] = get_output([img])\n",
    "conv_outputs = conv_outputs[0,:,:,:]\n",
    "print(conv_outputs.shape)\n",
    "print(class_weights.shape)\n",
    "\n",
    "def make_cam(conv_outputs, class_weights, original_shape, target_class):\n",
    "    cam = np.zeros(dtype=np.float32, shape = conv_outputs.shape[0:2])\n",
    "    for i, w in enumerate(class_weights[:, target_class]):\n",
    "        cam += w * conv_outputs[:,:,i]\n",
    "    cam /= np.max(cam)\n",
    "    return cv2.resize(cam, (28, 28))\n",
    "\n",
    "def make_heatmap(cam):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "    heatmap[np.where(cam < 0.1)] = 0\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "cam = make_cam(conv_outputs, class_weights, original_shape=(28,28), target_class=2)\n",
    "false_cam = make_cam(conv_outputs, class_weights, original_shape=(28,28), target_class=4)\n",
    "false2_cam = make_cam(conv_outputs, class_weights, original_shape=(28,28), target_class=5)\n",
    "\n",
    "heatmap = make_heatmap(cam)\n",
    "false_heatmap = make_heatmap(false_cam)\n",
    "false2_heatmap = make_heatmap(false2_cam)\n",
    "\n",
    "new_img = heatmap*0.5 + img\n",
    "final_img = new_img.reshape((28,28,3))\n",
    "\n",
    "# f, axarr = plt.subplots(2,1)\n",
    "# axarr[0,0].imshow(heatmap)\n",
    "# axarr[0,1].imshow(img.reshape(28,28))\n",
    "\n",
    "imgs = [heatmap, img.reshape(28,28)]\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 15))\n",
    "axes[0].imshow(heatmap)\n",
    "axes[0].set_title(\"Activation Map for 2\")\n",
    "axes[1].imshow(false_heatmap)\n",
    "axes[1].set_title(\"Activation Map for 4\")\n",
    "axes[2].imshow(false2_heatmap)\n",
    "axes[2].set_title(\"Activation Map for 5\")\n",
    "axes[3].imshow(img.reshape((28,28)))\n",
    "axes[3].set_title(\"True Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x189f7aee0b8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 24)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam /= np.max(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_28_1/Relu:0' shape=(?, 8, 8, 64) dtype=float32>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "\n",
    "dense_weights = model.layers[-2].get_weights()[0]\n",
    "\n",
    "softmax_weights = model.layers[-1].get_weights()[0]\n",
    "\n",
    "dense_weights.shape\n",
    "softmax_weights.shape\n",
    "\n",
    "final_conv_layer = get_output_layer(model, \"conv2d_28\")\n",
    "final_conv_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights is shape (128, 10)\n",
      "Conv2D output shape: (1, 8, 8, 64)\n",
      "Predictions: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "class_weights: np.ndarray = model.layers[-1].get_weights()[0] # class weights is of shape 32 x 10 (number of filter outputs x classes)\n",
    "print(f\"Class weights is shape {class_weights.shape}\")\n",
    "final_conv_layer: Conv2D = get_output_layer(model, \"conv2d_28\")\n",
    "\n",
    "input_tensor: Tensor = model.layers[0].input\n",
    "final_conv_layer_output: Tensor = final_conv_layer.output\n",
    "model_class_weights: Tensor = model.layers[-1].output\n",
    "    \n",
    "# K.function is a function factory that accepts arbitrary input layers and outputs arbitrary output layers\n",
    "get_output = K.function([input_tensor], [final_conv_layer_output, model_class_weights])\n",
    "\n",
    "[conv_outputs, predictions] = get_output([img])\n",
    "print(\"Conv2D output shape:\", conv_outputs.shape) # should match the shape of the outputs from the Conv2D layer\n",
    "print(\"Predictions:\", predictions.shape)\n",
    "np.argmax(predictions)\n",
    "\n",
    "conv_outputs = conv_outputs[0,:,:,:]\n",
    "# [conv_outputs, predictions] = get_output([img])\n",
    "# conv_outputs = conv_outputs[0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the class activation map\n",
    "\n",
    "class_activation_map = np.zeros(dtype=np.float32, shape=conv_outputs.shape[1:3])\n",
    "class_activation_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape to the network input shape (3, w, h).\n",
    "img = np.array([np.transpose(np.float32(original_img), (2, 0, 1))])\n",
    "\n",
    "#Get the 512 input weights to the softmax.\n",
    "class_weights = model.layers[-1].get_weights()[0]\n",
    "final_conv_layer = get_output_layer(model, \"conv5_3\")\n",
    "get_output = K.function([model.layers[0].input], \\\n",
    "            [final_conv_layer.output, \n",
    "model.layers[-1].output])\n",
    "[conv_outputs, predictions] = get_output([img])\n",
    "conv_outputs = conv_outputs[0, :, :, :]\n",
    "\n",
    "#Create the class activation map.\n",
    "cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\n",
    "target_class = 1\n",
    "for i, w in enumerate(class_weights[:, target_class]):\n",
    "        cam += w * conv_outputs[i, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything Below This Section Is Doodling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \n",
    "original_img = cv2.imread(image_path, 1)\n",
    "width, height, _ = original_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vanilla_cnn(filters_layer1:int, filters_layer2:int, kernel_size:int, input_dims: GrayScaleImageShape)-> Model:\n",
    "    inputs: Tensor = Input(shape=input_dims)\n",
    "    x: Tensor = Conv2D(filters=filters_layer1, kernel_size=kernel_size, activation='relu')(inputs)\n",
    "    x: Tensor = Conv2D(filters=filters_layer2, kernel_size=kernel_size, activation='relu')(x)\n",
    "    x: Tensor = build_global_average_pooling_layer(global_average_pooling, )\n",
    "    predictions = Dense(K, activation=\"softmax\")(x)\n",
    "    print(predictions)\n",
    "\n",
    "    #compile model using accuracy to measure model performance\n",
    "    model: Model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-3de1ded8ceb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'attention_vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    864\u001b[0m             \u001b[1;34m'kernel_constraint'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;34m'bias_constraint'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m         }\n\u001b[0m\u001b[0;32m    867\u001b[0m         \u001b[0mbase_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlegacy_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         return {\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[1;34m'scale'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m             \u001b[1;34m'mode'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;34m'distribution'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "from keras.layers import merge\n",
    "\n",
    "def build_model(input_dim):\n",
    "    inputs = Input(shape=input_dim)\n",
    "\n",
    "    # ATTENTION PART STARTS HERE\n",
    "    attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)\n",
    "    attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')\n",
    "    # ATTENTION PART FINISHES HERE\n",
    "\n",
    "    attention_mul = Dense(64)(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "\n",
    "inputs = Input(shape=input_dims)\n",
    "attention_probs = Dense(input_dims, activation='softmax', name='attention_vec')(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding shape from (60000, 28, 28) to (60000, 28, 28, 1)\n",
      "Expanding shape from (10000, 28, 28) to (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train.reshape((60000,1,28,28))\n",
    "\n",
    "def expand_tensor_shape(X_train: np.ndarray)-> np.ndarray:\n",
    "    new_shape: Tuple = X_train.shape + (1,)\n",
    "    print(f\"Expanding shape from {X_train.shape} to {new_shape}\")\n",
    "    return X_train.reshape(new_shape)\n",
    "\n",
    "X_train_expanded: np.ndarray = expand_tensor_shape(X_train)\n",
    "X_test_expanded: np.ndarray = expand_tensor_shape(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEI Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.JpegImagePlugin.JpegImageFile"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL.JpegImagePlugin import JpegImageFile\n",
    "\n",
    "image: JpegImageFile = load_img('1-01.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
